
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">

<html>
<head>
  <title></title>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">

  <style type="text/css">
body {
}
h1 {
    background: #f3f3f3;
    margin: 0px;
    text-align: center;
    padding: 10px;
}
div.buttons {
    width: 100%;
    display: flex;
    justify-content: center;
}
div.header-outer {
    background: #f3f3f3;
    display: flex;
    flex-direction: column;
    align-items: center;
    line-height: 130%;
    margin: 0px;
    padding: 10px;
    font-size: large;
}
div.header {
    max-width: 1000px;
}
.button {
    cursor: pointer;
    border: 10px;
    margin: 5px;
    padding: 15px 32px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
}
#dybutton {
    background-color: #f8fad2;
}
#tfbutton {
    background-color: #d0eaf0;
}
#ptbutton { 
    background-color: #fbe1d3;
}
div.disqus {
    max-width: 1000px;
    margin: auto;
}

div.main {
    text-align: center;
    padding-top: 10px;
}
div.outer {
    white-space: nowrap;
    display: none;
}
span.description {
    text-align: left;
    vertical-align: top;
    font-size: large;
    line-height: 112%;
    width: 400px;
    white-space: normal;
    margin-left: 5px;
    margin-right: 5px;
    display: none;
}
span.description.dynet {
    background-color: #f8fad2;
    display: none;
}
span.description.pytorch {
    background-color: #fbe1d3;
    display: none;
}
span.description.tensorflow {
    background-color: #d0eaf0;
    display: none;
}
code {
    text-align: left;
    vertical-align: top;
    width: 100ch;
    display: none;
}
pre { }
td.linenos { background-color: #f0f0f0; padding-right: 10px; }
span.lineno { background-color: #f0f0f0; padding: 0 5px 0 5px; }
pre { line-height: 125%; font-family: Menlo, "Courier New", Courier, monospace; margin: 0 }
body .bp { color: #0a5ac2 } /* Name.Builtin.Pseudo */
body .c1 { color: #6a727c } /* Comment.Single */
body .fm { color: #0a5ac2 } /* Name.Function.Magic */
body .k { color: #d63e4d } /* Keyword */
body .kn { color: #d63e4d; } /* Keyword.Namespace */
body .mf { color: #0a5ac2 } /* Literal.Number.Float */
body .mi { color: #0a5ac2 } /* Literal.Number.Integer */
body .n { color: #000000 } /* Variable */
body .nb { color: #0a5ac2 } /* Name.Builtin */
body .nc { color: #6f40bf } /* Name.Class */
body .nf { color: #6f40bf } /* Name.Function */
body .nn { color: #000000 } /* Name.Namespace */
body .o { color: #d63e4d } /* Operator */
body .ow { color: #d63e4d } /* Operator.Word */
body .p { color: #000000 } /* Parentheses */
body .s1 { color: #052e60 } /* Literal.String.Single */
body .s2 { color: #052e60 } /* Literal.String.Double */
body .sd { color: #052e60 } /* Literal.String.Doc */
body .vm { color: #0a5ac2 } 

body .c { color: #999988 } /* Comment */
body .cm { color: #6a727c } /* Comment.Multiline */
body .cp { color: #6a727c } /* Comment.Preproc */
body .cs { color: #6a727c } /* Comment.Special */
body .err { color: #a61717 } /* Error */
body .gd { color: #000000 } /* Generic.Deleted */
body .ge { color: #000000 } /* Generic.Emph */
body .gh { color: #999999 } /* Generic.Heading */
body .gi { color: #000000 } /* Generic.Inserted */
body .go { color: #888888 } /* Generic.Output */
body .gp { color: #555555 } /* Generic.Prompt */
body .gr { color: #aa0000 } /* Generic.Error */
body .gs { } /* Generic.Strong */
body .gt { color: #aa0000 } /* Generic.Traceback */
body .gu { color: #aaaaaa } /* Generic.Subheading */
body .hll { }
body .kc { color: #000000; } /* Keyword.Constant */
body .kd { color: #000000; } /* Keyword.Declaration */
body .kp { color: #000000; } /* Keyword.Pseudo */
body .kr { color: #000000; } /* Keyword.Reserved */
body .kt { color: #445588; } /* Keyword.Type */
body .m { color: #009999 } /* Literal.Number */
body .no { color: #008080 } /* Name.Constant */
body .s { color: #d01040 } /* Literal.String */
body .na { color: #008080 } /* Name.Attribute */
body .nd { color: #3c5d5d } /* Name.Decorator */
body .ni { color: #800080 } /* Name.Entity */
body .ne { color: #990000 } /* Name.Exception */
body .nl { color: #990000 } /* Name.Label */
body .nt { color: #000080 } /* Name.Tag */
body .nv { color: #008080 } /* Name.Variable */
body .w { color: #bbbbbb } /* Text.Whitespace */
body .mh { color: #0a5ac2 } /* Literal.Number.Hex */
body .mo { color: #0a5ac2 } /* Literal.Number.Oct */
body .sb { color: #052e60 } /* Literal.String.Backtick */
body .sc { color: #052e60 } /* Literal.String.Char */
body .se { color: #052e60 } /* Literal.String.Escape */
body .sh { color: #052e60 } /* Literal.String.Heredoc */
body .si { color: #052e60 } /* Literal.String.Interpol */
body .sx { color: #052e60 } /* Literal.String.Other */
body .sr { color: #052e60 } /* Literal.String.Regex */
body .ss { color: #052e60 } /* Literal.String.Symbol */
body .vc { color: #008080 } /* Name.Variable.Class */
body .vg { color: #008080 } /* Name.Variable.Global */
body .vi { color: #008080 } /* Name.Variable.Instance */
body .il { color: #009999 } /* Literal.Number.Integer.Long */
</style>

</head>

<body>
<h1>Implementing a neural Part-of-Speech tagger</h1>
<div class="header-outer">
<div class=header>
<p>
DyNet, PyTorch and Tensorflow are complex frameworks with different ways of approaching neural network implementation and variations in default behaviour.
This page is intended to show how to implement the same non-trivial model in all three.
The design of the page is motivated by my own preference for a complete program with annotations, rather than the more common tutorial style of introducing code piecemeal in between discussion.
The design of the code is also geared towards providing a complete picture of how things fit together.
For a non-tutorial version of this code it would be better to use abstraction to improve flexibility, but that would have complicated the flow here.
</p>
<p>
Use the buttons to show one or more implementations and their associated comments (note, depending on your screen size you may need to scroll to see all the code).
Matching or closely related content is aligned.
Framework-specific comments are highlighted in a colour that matches their button and a line is used to make the link from the comment to the code clear.
The <a href="https://github.com/jkkummerfeld/neural-tagger-tutorial">repository</a> for this page provides the code in runnable form.
The only dependencies are the respective frameworks (DyNet <a href="https://github.com/clab/dynet/releases/tag/2.0.3">2.0.3</a>, PyTorch <a href="https://github.com/pytorch/pytorch/releases/tag/v0.4.1">0.4.1</a> and Tensorflow <a href="https://github.com/tensorflow/tensorflow/releases/tag/v1.9.0">1.9.0</a>).
</p>
<p>
The three implementations below all produce part-of-speech taggers that score ~97.2% on the development set of the Penn Treebank.
The specific hyperparameter choices follows <a href="https://arxiv.org/abs/1806.04470">Yang, Liang, and Zhang (CoLing 2018)</a> and matches their performance for the setting without a CRF layer or character-based word embeddings.
</p>
<p>
Making this helped me understand all three frameworks better. Hopefully you will find it informative too!
</p>

<div class="buttons">
<button class="button" id="dybutton" onmouseover="" onclick="toggleDyNet()">Show/Hide DyNet</button>
<button class="button" id="ptbutton" onmouseover="" onclick="togglePyTorch()">Show/Hide PyTorch</button>
<button class="button" id="tfbutton" onmouseover="" onclick="toggleTensorflow()">Show/Hide Tensorflow</button>
</div>
</div>

</div>


<div class="main">
<div class="outer shared-content">
<span class="description shared-content left">Imports
<br />
We use argparse for processing command line arguments, random for shuffling our data, sys for flushing output, and numpy for handling vectors of data.<br /><br /></span><code class="dynet"><pre><span></span><span class="c1"># DyNet Implementation</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

</pre></code><span class="description shared-content centre">Imports
<br />
We use argparse for processing command line arguments, random for shuffling our data, sys for flushing output, and numpy for handling vectors of data.<br /><br /></span><code class="pytorch"><pre><span></span><span class="c1"># PyTorch Implementation</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

</pre></code><span class="description shared-content right">Imports
<br />
We use argparse for processing command line arguments, random for shuffling our data, sys for flushing output, and numpy for handling vectors of data.<br /><br /></span><code class="tensorflow"><pre><span></span><span class="c1"># Tensorflow Implementation</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content left">Constants
<br />
Typically, we would make many of these command line arguments and tune using the development set. For simplicity, I have fixed their values here to match Jiang, Liang and Zhang (CoLing 2018).<br /><br /></span><code class="dynet"><pre><span></span><span class="n">PAD</span> <span class="o">=</span> <span class="s2">&quot;__PAD__&quot;</span>
<span class="n">UNK</span> <span class="o">=</span> <span class="s2">&quot;__UNK__&quot;</span>
<span class="n">DIM_EMBEDDING</span> <span class="o">=</span> <span class="mi">100</span> 
<span class="n">LSTM_HIDDEN</span> <span class="o">=</span> <span class="mi">100</span> 
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">10</span> 
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.015</span> 
<span class="n">LEARNING_DECAY_RATE</span> <span class="o">=</span> <span class="mf">0.05</span> 
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">100</span> 
<span class="n">KEEP_PROB</span> <span class="o">=</span> <span class="mf">0.5</span> 
<span class="n">GLOVE</span> <span class="o">=</span> <span class="s2">&quot;../data/glove.6B.100d.txt&quot;</span> 
<span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">1e-8</span>

</pre></code><span class="description shared-content centre">Constants
<br />
Typically, we would make many of these command line arguments and tune using the development set. For simplicity, I have fixed their values here to match Jiang, Liang and Zhang (CoLing 2018).<br /><br /></span><code class="pytorch"><pre><span></span><span class="n">PAD</span> <span class="o">=</span> <span class="s2">&quot;__PAD__&quot;</span>
<span class="n">UNK</span> <span class="o">=</span> <span class="s2">&quot;__UNK__&quot;</span>
<span class="n">DIM_EMBEDDING</span> <span class="o">=</span> <span class="mi">100</span> 
<span class="n">LSTM_HIDDEN</span> <span class="o">=</span> <span class="mi">100</span> 
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">10</span> 
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.015</span> 
<span class="n">LEARNING_DECAY_RATE</span> <span class="o">=</span> <span class="mf">0.05</span> 
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">100</span> 
<span class="n">KEEP_PROB</span> <span class="o">=</span> <span class="mf">0.5</span> 
<span class="n">GLOVE</span> <span class="o">=</span> <span class="s2">&quot;../data/glove.6B.100d.txt&quot;</span> 
<span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">1e-8</span>

</pre></code><span class="description shared-content right">Constants
<br />
Typically, we would make many of these command line arguments and tune using the development set. For simplicity, I have fixed their values here to match Jiang, Liang and Zhang (CoLing 2018).<br /><br /></span><code class="tensorflow"><pre><span></span><span class="n">PAD</span> <span class="o">=</span> <span class="s2">&quot;__PAD__&quot;</span>
<span class="n">UNK</span> <span class="o">=</span> <span class="s2">&quot;__UNK__&quot;</span>
<span class="n">DIM_EMBEDDING</span> <span class="o">=</span> <span class="mi">100</span> 
<span class="n">LSTM_HIDDEN</span> <span class="o">=</span> <span class="mi">100</span> 
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">10</span> 
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.015</span> 
<span class="n">LEARNING_DECAY_RATE</span> <span class="o">=</span> <span class="mf">0.05</span> 
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">100</span> 
<span class="n">KEEP_PROB</span> <span class="o">=</span> <span class="mf">0.5</span> 
<span class="n">GLOVE</span> <span class="o">=</span> <span class="s2">&quot;../data/glove.6B.100d.txt&quot;</span> 
<span class="c1"># WEIGHT_DECAY = 1e-8 See note</span>

</pre></code></div>
<div class="outer dynet">
<span class="description dynet left">DyNet specfic imports
<br />
The first allows us to configure DyNet from within code rather than on the command line:  mem is the amount of system memory initially allocated (DyNet has its own memory management), autobatch toggles automatic parallelisation of computations, weight_decay rescales weights by (1 - decay) after every update, random_seed sets the seed for random number generation.<br /><br /></span><code class="dynet"><pre><span></span><span class="kn">import</span> <span class="nn">dynet_config</span>
<span class="n">dynet_config</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">mem</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">autobatch</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span><span class="n">random_seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># dynet_config.set_gpu() </span>
<span class="kn">import</span> <span class="nn">dynet</span> <span class="kn">as</span> <span class="nn">dy</span>

</pre></code><span class="description dynet centre">DyNet specfic imports
<br />
The first allows us to configure DyNet from within code rather than on the command line:  mem is the amount of system memory initially allocated (DyNet has its own memory management), autobatch toggles automatic parallelisation of computations, weight_decay rescales weights by (1 - decay) after every update, random_seed sets the seed for random number generation.<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">DyNet specfic imports
<br />
The first allows us to configure DyNet from within code rather than on the command line:  mem is the amount of system memory initially allocated (DyNet has its own memory management), autobatch toggles automatic parallelisation of computations, weight_decay rescales weights by (1 - decay) after every update, random_seed sets the seed for random number generation.<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">PyTorch specfic imports<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">PyTorch specfic imports<br /><br /></span><code class="pytorch"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

</pre></code><span class="description pytorch right">PyTorch specfic imports<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Tensorflow specfic import<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Tensorflow specfic import<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Tensorflow specfic import<br /><br /></span><code class="tensorflow"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content left">Data reading
<br />
We are expecting a minor variation on the raw Penn Treebank data, with one line per sentence, tokens separated by spaces, and the tag for each token placed next to its word (the | works as a separator as it does not appear as a token).<br /><br /></span><code class="dynet"><pre><span></span><span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Example input:</span>
<span class="sd">    Pierre|NNP Vinken|NNP ,|, 61|CD years|NNS old|JJ</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">content</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">data_src</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">data_src</span><span class="p">:</span>
            <span class="n">t_p</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;|&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t_p</span><span class="p">]</span>
            <span class="n">tags</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t_p</span><span class="p">]</span>
            <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">tokens</span><span class="p">,</span> <span class="n">tags</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">content</span>

</pre></code><span class="description shared-content centre">Data reading
<br />
We are expecting a minor variation on the raw Penn Treebank data, with one line per sentence, tokens separated by spaces, and the tag for each token placed next to its word (the | works as a separator as it does not appear as a token).<br /><br /></span><code class="pytorch"><pre><span></span><span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Example input:</span>
<span class="sd">    Pierre|NNP Vinken|NNP ,|, 61|CD years|NNS old|JJ</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">content</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">data_src</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">data_src</span><span class="p">:</span>
            <span class="n">t_p</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;|&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t_p</span><span class="p">]</span>
            <span class="n">tags</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t_p</span><span class="p">]</span>
            <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">tokens</span><span class="p">,</span> <span class="n">tags</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">content</span>

</pre></code><span class="description shared-content right">Data reading
<br />
We are expecting a minor variation on the raw Penn Treebank data, with one line per sentence, tokens separated by spaces, and the tag for each token placed next to its word (the | works as a separator as it does not appear as a token).<br /><br /></span><code class="tensorflow"><pre><span></span><span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Example input:</span>
<span class="sd">    Pierre|NNP Vinken|NNP ,|, 61|CD years|NNS old|JJ</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">content</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">data_src</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">data_src</span><span class="p">:</span>
            <span class="n">t_p</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;|&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t_p</span><span class="p">]</span>
            <span class="n">tags</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t_p</span><span class="p">]</span>
            <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">tokens</span><span class="p">,</span> <span class="n">tags</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">content</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content left">Simplificaiton by replacing all digits with 0 to decrease sparsity.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span><span class="k">def</span> <span class="nf">simplify_token</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>
    <span class="n">chars</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">token</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">char</span><span class="o">.</span><span class="n">isdigit</span><span class="p">():</span>
            <span class="n">chars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;0&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
</pre></div>
</code><span class="description shared-content centre">Simplificaiton by replacing all digits with 0 to decrease sparsity.<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span><span class="k">def</span> <span class="nf">simplify_token</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>
    <span class="n">chars</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">token</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">char</span><span class="o">.</span><span class="n">isdigit</span><span class="p">():</span>
            <span class="n">chars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;0&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
</pre></div>
</code><span class="description shared-content right">Simplificaiton by replacing all digits with 0 to decrease sparsity.<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span><span class="k">def</span> <span class="nf">simplify_token</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>
    <span class="n">chars</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">token</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">char</span><span class="o">.</span><span class="n">isdigit</span><span class="p">():</span>
            <span class="n">chars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;0&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content left">Read arguments
<br />
For the purpose of this example we only have arguments for locations of the data.<br /><br /></span><code class="dynet"><pre><span></span>    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;POS tagger.&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;training_data&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;dev_data&#39;</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">train</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">training_data</span><span class="p">)</span>
    <span class="n">dev</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">dev_data</span><span class="p">)</span>

</pre></code><span class="description shared-content centre">Read arguments
<br />
For the purpose of this example we only have arguments for locations of the data.<br /><br /></span><code class="pytorch"><pre><span></span>    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;POS tagger.&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;training_data&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;dev_data&#39;</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">train</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">training_data</span><span class="p">)</span>
    <span class="n">dev</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">dev_data</span><span class="p">)</span>

</pre></code><span class="description shared-content right">Read arguments
<br />
For the purpose of this example we only have arguments for locations of the data.<br /><br /></span><code class="tensorflow"><pre><span></span>    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;POS tagger.&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;training_data&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;dev_data&#39;</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">train</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">training_data</span><span class="p">)</span>
    <span class="n">dev</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">dev_data</span><span class="p">)</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content left">Make indices
<br />
These are mappings from strings to integers that will be used to get the input for our model and to process the output. UNK is added to our mapping so that there is a vector we can use when we encounter unknown words. The special PAD symbol is used in PyTorch and Tensorflow as part of shaping the data in a batch to be a consistent size. It is not needed for DyNet, but kept for consistency.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>    <span class="n">id_to_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">PAD</span><span class="p">,</span> <span class="n">UNK</span><span class="p">]</span>
    <span class="n">token_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">UNK</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="n">id_to_tag</span> <span class="o">=</span> <span class="p">[</span><span class="n">PAD</span><span class="p">]</span>
    <span class="n">tag_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
</pre></div>
</code><span class="description shared-content centre">Make indices
<br />
These are mappings from strings to integers that will be used to get the input for our model and to process the output. UNK is added to our mapping so that there is a vector we can use when we encounter unknown words. The special PAD symbol is used in PyTorch and Tensorflow as part of shaping the data in a batch to be a consistent size. It is not needed for DyNet, but kept for consistency.<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>    <span class="n">id_to_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">PAD</span><span class="p">,</span> <span class="n">UNK</span><span class="p">]</span>
    <span class="n">token_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">UNK</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="n">id_to_tag</span> <span class="o">=</span> <span class="p">[</span><span class="n">PAD</span><span class="p">]</span>
    <span class="n">tag_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
</pre></div>
</code><span class="description shared-content right">Make indices
<br />
These are mappings from strings to integers that will be used to get the input for our model and to process the output. UNK is added to our mapping so that there is a vector we can use when we encounter unknown words. The special PAD symbol is used in PyTorch and Tensorflow as part of shaping the data in a batch to be a consistent size. It is not needed for DyNet, but kept for consistency.<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>    <span class="n">id_to_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">PAD</span><span class="p">,</span> <span class="n">UNK</span><span class="p">]</span>
    <span class="n">token_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">UNK</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="n">id_to_tag</span> <span class="o">=</span> <span class="p">[</span><span class="n">PAD</span><span class="p">]</span>
    <span class="n">tag_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content left">dev is necessary here to get the GloVe embeddings for words in dev but not train loaded. They will not be updated during training as they do not occur.<br /><br /></span><code class="dynet"><pre><span></span>    <span class="k">for</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">tags</span> <span class="ow">in</span> <span class="n">train</span> <span class="o">+</span> <span class="n">dev</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
            <span class="n">token</span> <span class="o">=</span> <span class="n">simplify_token</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">token_to_id</span><span class="p">:</span>
                <span class="n">token_to_id</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_to_id</span><span class="p">)</span>
                <span class="n">id_to_token</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">tags</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">tag</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tag_to_id</span><span class="p">:</span>
                <span class="n">tag_to_id</span><span class="p">[</span><span class="n">tag</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tag_to_id</span><span class="p">)</span>
                <span class="n">id_to_tag</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span>
    <span class="n">NWORDS</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_to_id</span><span class="p">)</span>
    <span class="n">NTAGS</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tag_to_id</span><span class="p">)</span>

</pre></code><span class="description shared-content centre">dev is necessary here to get the GloVe embeddings for words in dev but not train loaded. They will not be updated during training as they do not occur.<br /><br /></span><code class="pytorch"><pre><span></span>    <span class="k">for</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">tags</span> <span class="ow">in</span> <span class="n">train</span> <span class="o">+</span> <span class="n">dev</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
            <span class="n">token</span> <span class="o">=</span> <span class="n">simplify_token</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">token_to_id</span><span class="p">:</span>
                <span class="n">token_to_id</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_to_id</span><span class="p">)</span>
                <span class="n">id_to_token</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">tags</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">tag</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tag_to_id</span><span class="p">:</span>
                <span class="n">tag_to_id</span><span class="p">[</span><span class="n">tag</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tag_to_id</span><span class="p">)</span>
                <span class="n">id_to_tag</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span>
    <span class="n">NWORDS</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_to_id</span><span class="p">)</span>
    <span class="n">NTAGS</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tag_to_id</span><span class="p">)</span>

</pre></code><span class="description shared-content right">dev is necessary here to get the GloVe embeddings for words in dev but not train loaded. They will not be updated during training as they do not occur.<br /><br /></span><code class="tensorflow"><pre><span></span>    <span class="k">for</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">tags</span> <span class="ow">in</span> <span class="n">train</span> <span class="o">+</span> <span class="n">dev</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
            <span class="n">token</span> <span class="o">=</span> <span class="n">simplify_token</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">token_to_id</span><span class="p">:</span>
                <span class="n">token_to_id</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_to_id</span><span class="p">)</span>
                <span class="n">id_to_token</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">tags</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">tag</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tag_to_id</span><span class="p">:</span>
                <span class="n">tag_to_id</span><span class="p">[</span><span class="n">tag</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tag_to_id</span><span class="p">)</span>
                <span class="n">id_to_tag</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span>
    <span class="n">NWORDS</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_to_id</span><span class="p">)</span>
    <span class="n">NTAGS</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tag_to_id</span><span class="p">)</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content left">Load pre-trained vectors
<br />
I am assuming these are the 100-dimensional GloVe embeddings in their standard format.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>    <span class="n">pretrained</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="n">GLOVE</span><span class="p">):</span>
        <span class="n">parts</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">vector</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
        <span class="n">pretrained</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector</span>
</pre></div>
</code><span class="description shared-content centre">Load pre-trained vectors
<br />
I am assuming these are the 100-dimensional GloVe embeddings in their standard format.<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>    <span class="n">pretrained</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="n">GLOVE</span><span class="p">):</span>
        <span class="n">parts</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">vector</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
        <span class="n">pretrained</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector</span>
</pre></div>
</code><span class="description shared-content right">Load pre-trained vectors
<br />
I am assuming these are the 100-dimensional GloVe embeddings in their standard format.<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>    <span class="n">pretrained</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="n">GLOVE</span><span class="p">):</span>
        <span class="n">parts</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">vector</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
        <span class="n">pretrained</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content left">We need the word vectors as a list to initialise the embeddings. Each entry in the list corresponds to the token with that index.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>    <span class="n">pretrained_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">3.0</span> <span class="o">/</span> <span class="n">DIM_EMBEDDING</span><span class="p">)</span> 
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">id_to_token</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">pretrained</span><span class="p">:</span> 
            <span class="n">pretrained_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pretrained</span><span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()]))</span>
        <span class="k">else</span><span class="p">:</span>
</pre></div>
</code><span class="description shared-content centre">We need the word vectors as a list to initialise the embeddings. Each entry in the list corresponds to the token with that index.<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>    <span class="n">pretrained_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">3.0</span> <span class="o">/</span> <span class="n">DIM_EMBEDDING</span><span class="p">)</span> 
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">id_to_token</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">pretrained</span><span class="p">:</span> 
            <span class="n">pretrained_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pretrained</span><span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()]))</span>
        <span class="k">else</span><span class="p">:</span>
</pre></div>
</code><span class="description shared-content right">We need the word vectors as a list to initialise the embeddings. Each entry in the list corresponds to the token with that index.<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>    <span class="n">pretrained_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">3.0</span> <span class="o">/</span> <span class="n">DIM_EMBEDDING</span><span class="p">)</span> 
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">id_to_token</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">pretrained</span><span class="p">:</span> 
            <span class="n">pretrained_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pretrained</span><span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()]))</span>
        <span class="k">else</span><span class="p">:</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content left">For words that do not appear in GloVe we generate a random vector (note, the choice of scale here is important).<br /><br /></span><code class="dynet"><pre><span></span>            <span class="n">random_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="p">[</span><span class="n">DIM_EMBEDDING</span><span class="p">])</span>
            <span class="n">pretrained_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">random_vector</span><span class="p">)</span>

</pre></code><span class="description shared-content centre">For words that do not appear in GloVe we generate a random vector (note, the choice of scale here is important).<br /><br /></span><code class="pytorch"><pre><span></span>            <span class="n">random_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="p">[</span><span class="n">DIM_EMBEDDING</span><span class="p">])</span>
            <span class="n">pretrained_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">random_vector</span><span class="p">)</span>

</pre></code><span class="description shared-content right">For words that do not appear in GloVe we generate a random vector (note, the choice of scale here is important).<br /><br /></span><code class="tensorflow"><pre><span></span>            <span class="n">random_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="p">[</span><span class="n">DIM_EMBEDDING</span><span class="p">])</span>
            <span class="n">pretrained_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">random_vector</span><span class="p">)</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content left">&nbsp;</span><code class="dynet"><div class="source"><pre><span></span>    <span class="c1"># DyNet model creation</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">dy</span><span class="o">.</span><span class="n">ParameterCollection</span><span class="p">()</span>
</pre></div>
</code><span class="description shared-content centre">&nbsp;</span><code class="pytorch">&nbsp;</code><span class="description shared-content right">&nbsp;</span><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet left">Lookup parameters are a matrix that supports efficient sparse lookup.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>    <span class="n">pEmbedding</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">add_lookup_parameters</span><span class="p">((</span><span class="n">NWORDS</span><span class="p">,</span> <span class="n">DIM_EMBEDDING</span><span class="p">))</span>
    <span class="n">pEmbedding</span><span class="o">.</span><span class="n">init_from_array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pretrained_list</span><span class="p">))</span>
</pre></div>
</code><span class="description dynet centre">Lookup parameters are a matrix that supports efficient sparse lookup.<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">Lookup parameters are a matrix that supports efficient sparse lookup.<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet left">Objects that create LSTM cells and the necessary parameters.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>    <span class="n">stdv</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">LSTM_HIDDEN</span><span class="p">)</span> 
    <span class="n">f_lstm</span> <span class="o">=</span> <span class="n">dy</span><span class="o">.</span><span class="n">VanillaLSTMBuilder</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">DIM_EMBEDDING</span><span class="p">,</span> <span class="n">LSTM_HIDDEN</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span>
            <span class="n">forget_bias</span><span class="o">=</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_sample</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">stdv</span><span class="p">)</span>
    <span class="n">b_lstm</span> <span class="o">=</span> <span class="n">dy</span><span class="o">.</span><span class="n">VanillaLSTMBuilder</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">DIM_EMBEDDING</span><span class="p">,</span> <span class="n">LSTM_HIDDEN</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span>
            <span class="n">forget_bias</span><span class="o">=</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_sample</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">stdv</span><span class="p">)</span>
</pre></div>
</code><span class="description dynet centre">Objects that create LSTM cells and the necessary parameters.<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">Objects that create LSTM cells and the necessary parameters.<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet left">A simple weight matrix for the final output calculation.<br /><br /></span><code class="dynet"><pre><span></span>    <span class="n">pOutput</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">add_parameters</span><span class="p">((</span><span class="n">NTAGS</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">LSTM_HIDDEN</span><span class="p">))</span>

</pre></code><span class="description dynet centre">A simple weight matrix for the final output calculation.<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">A simple weight matrix for the final output calculation.<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet left">Setting recurrent dropout values (not used in this case).<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>    <span class="n">f_lstm</span><span class="o">.</span><span class="n">set_dropouts</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="n">b_lstm</span><span class="o">.</span><span class="n">set_dropouts</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</pre></div>
</code><span class="description dynet centre">Setting recurrent dropout values (not used in this case).<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">Setting recurrent dropout values (not used in this case).<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet left">To match PyTorch, we initialise the parameters with an unconventional approach.<br /><br /></span><code class="dynet"><pre><span></span>    <span class="n">f_lstm</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">stdv</span><span class="p">,</span> <span class="n">stdv</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">LSTM_HIDDEN</span><span class="p">,</span> <span class="n">DIM_EMBEDDING</span><span class="p">]))</span>
    <span class="n">f_lstm</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">stdv</span><span class="p">,</span> <span class="n">stdv</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">LSTM_HIDDEN</span><span class="p">,</span> <span class="n">LSTM_HIDDEN</span><span class="p">]))</span>
    <span class="n">f_lstm</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">stdv</span><span class="p">,</span> <span class="n">stdv</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">LSTM_HIDDEN</span><span class="p">]))</span>
    <span class="n">b_lstm</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">stdv</span><span class="p">,</span> <span class="n">stdv</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">LSTM_HIDDEN</span><span class="p">,</span> <span class="n">DIM_EMBEDDING</span><span class="p">]))</span>
    <span class="n">b_lstm</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">stdv</span><span class="p">,</span> <span class="n">stdv</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">LSTM_HIDDEN</span><span class="p">,</span> <span class="n">LSTM_HIDDEN</span><span class="p">]))</span>
    <span class="n">b_lstm</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">stdv</span><span class="p">,</span> <span class="n">stdv</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">LSTM_HIDDEN</span><span class="p">]))</span>

</pre></code><span class="description dynet centre">To match PyTorch, we initialise the parameters with an unconventional approach.<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">To match PyTorch, we initialise the parameters with an unconventional approach.<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet left">The trainer object is used to update the model.
<br />
DyNet clips gradients by default, which we disable here (this can have a big impact on performance).<br /><br /></span><code class="dynet"><pre><span></span>    <span class="n">trainer</span> <span class="o">=</span> <span class="n">dy</span><span class="o">.</span><span class="n">SimpleSGDTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">set_clip_threshold</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

</pre></code><span class="description dynet centre">The trainer object is used to update the model.
<br />
DyNet clips gradients by default, which we disable here (this can have a big impact on performance).<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">The trainer object is used to update the model.
<br />
DyNet clips gradients by default, which we disable here (this can have a big impact on performance).<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer shared-content">
<span class="description shared-content left">&nbsp;</span><code class="dynet">&nbsp;</code><span class="description shared-content centre">&nbsp;</span><code class="pytorch"><div class="source"><pre><span></span>    <span class="c1"># PyTorch model creation</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">TaggerModel</span><span class="p">(</span><span class="n">NWORDS</span><span class="p">,</span> <span class="n">NTAGS</span><span class="p">,</span> <span class="n">pretrained_list</span><span class="p">,</span> <span class="n">id_to_token</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">,</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">)</span>
</pre></div>
</code><span class="description shared-content right">&nbsp;</span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">The learning rate for each epoch is set by multiplying the inital rate by the factor produced by this function.<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">The learning rate for each epoch is set by multiplying the inital rate by the factor produced by this function.<br /><br /></span><code class="pytorch"><pre><span></span>    <span class="n">rescale_lr</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">epoch</span><span class="p">:</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">LEARNING_DECAY_RATE</span> <span class="o">*</span> <span class="n">epoch</span><span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span>
            <span class="n">lr_lambda</span><span class="o">=</span><span class="n">rescale_lr</span><span class="p">)</span>

</pre></code><span class="description pytorch right">The learning rate for each epoch is set by multiplying the inital rate by the factor produced by this function.<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer shared-content">
<span class="description shared-content left">&nbsp;</span><code class="dynet">&nbsp;</code><span class="description shared-content centre">&nbsp;</span><code class="pytorch">&nbsp;</code><span class="description shared-content right">&nbsp;</span><code class="tensorflow"><div class="source"><pre><span></span>    <span class="c1"># Tensorflow computation graph definition</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Placeholders are inputs/values that will be fed into the network each time it is run. We define their type and the shape (constant, 1D vector, 2D vector, etc). This includes what we normally think of as inputs (e.g. the tokens) as well as parameters we want to change at run time (e.g. the learning rate).<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Placeholders are inputs/values that will be fed into the network each time it is run. We define their type and the shape (constant, 1D vector, 2D vector, etc). This includes what we normally think of as inputs (e.g. the tokens) as well as parameters we want to change at run time (e.g. the learning rate).<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Placeholders are inputs/values that will be fed into the network each time it is run. We define their type and the shape (constant, 1D vector, 2D vector, etc). This includes what we normally think of as inputs (e.g. the tokens) as well as parameters we want to change at run time (e.g. the learning rate).<br /><br /></span><code class="tensorflow"><pre><span></span>        <span class="n">e_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input&#39;</span><span class="p">)</span>
        <span class="n">e_lengths</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;lengths&#39;</span><span class="p">)</span>
        <span class="n">e_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;mask&#39;</span><span class="p">)</span>
        <span class="n">e_gold_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">],</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;gold_output&#39;</span><span class="p">)</span>
        <span class="n">e_keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;keep_prob&#39;</span><span class="p">)</span>
        <span class="n">e_learning_rate</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">)</span>

</pre></code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">The embedding matrix is a variable (so they can shift in training), initialized with the vectors defined above.<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">The embedding matrix is a variable (so they can shift in training), initialized with the vectors defined above.<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">The embedding matrix is a variable (so they can shift in training), initialized with the vectors defined above.<br /><br /></span><code class="tensorflow"><pre><span></span>        <span class="n">glove_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pretrained_list</span><span class="p">))</span>
        <span class="n">e_embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;embedding&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">NWORDS</span><span class="p">,</span> <span class="n">DIM_EMBEDDING</span><span class="p">],</span>
                <span class="n">initializer</span><span class="o">=</span><span class="n">glove_init</span><span class="p">)</span>
        <span class="n">e_embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">e_embedding</span><span class="p">,</span> <span class="n">e_input</span><span class="p">)</span>

</pre></code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">We create an LSTM cell, then wrap it in a class that applies dropout.<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">We create an LSTM cell, then wrap it in a class that applies dropout.<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">We create an LSTM cell, then wrap it in a class that applies dropout.<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">e_cell_f</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">LSTM_HIDDEN</span><span class="p">)</span>
        <span class="n">e_cell_f</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">DropoutWrapper</span><span class="p">(</span><span class="n">e_cell_f</span><span class="p">,</span>
                <span class="n">input_keep_prob</span><span class="o">=</span><span class="n">e_keep_prob</span><span class="p">,</span> <span class="n">output_keep_prob</span><span class="o">=</span><span class="n">e_keep_prob</span><span class="p">)</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Recurrent dropout options<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Recurrent dropout options<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Recurrent dropout options<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>        <span class="c1">#        variational_recurrent=True, dtype=tf.float32,</span>
        <span class="c1">#        input_size=DIM_EMBEDDING)</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">For a multi-layer network we would wrap a list of cells with MultiRNNCell<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">For a multi-layer network we would wrap a list of cells with MultiRNNCell<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">For a multi-layer network we would wrap a list of cells with MultiRNNCell<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>        <span class="c1"># e_cell_f = tf.contrib.rnn.MultiRNNCell([e_cell_f])</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Make a cell for the reverse direction<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Make a cell for the reverse direction<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Make a cell for the reverse direction<br /><br /></span><code class="tensorflow"><pre><span></span>        <span class="n">e_cell_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">LSTM_HIDDEN</span><span class="p">)</span>
        <span class="n">e_cell_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">DropoutWrapper</span><span class="p">(</span><span class="n">e_cell_b</span><span class="p">,</span>
                <span class="n">input_keep_prob</span><span class="o">=</span><span class="n">e_keep_prob</span><span class="p">,</span> <span class="n">output_keep_prob</span><span class="o">=</span><span class="n">e_keep_prob</span><span class="p">)</span>

</pre></code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">To use the cells we create a dynamic RNN. The 'dynamic' aspect means we can feed in the lengths of input sequences not counting padding and it will stop early.<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">To use the cells we create a dynamic RNN. The 'dynamic' aspect means we can feed in the lengths of input sequences not counting padding and it will stop early.<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">To use the cells we create a dynamic RNN. The 'dynamic' aspect means we can feed in the lengths of input sequences not counting padding and it will stop early.<br /><br /></span><code class="tensorflow"><pre><span></span>        <span class="n">e_initial_state_f</span> <span class="o">=</span> <span class="n">e_cell_f</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">e_initial_state_b</span> <span class="o">=</span> <span class="n">e_cell_f</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">e_lstm_outputs</span><span class="p">,</span> <span class="n">e_final_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bidirectional_dynamic_rnn</span><span class="p">(</span>
                <span class="n">cell_fw</span><span class="o">=</span><span class="n">e_cell_f</span><span class="p">,</span> <span class="n">cell_bw</span><span class="o">=</span><span class="n">e_cell_b</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">e_embed</span><span class="p">,</span>
                <span class="n">initial_state_fw</span><span class="o">=</span><span class="n">e_initial_state_f</span><span class="p">,</span>
                <span class="n">initial_state_bw</span><span class="o">=</span><span class="n">e_initial_state_b</span><span class="p">,</span>
                <span class="n">sequence_length</span><span class="o">=</span><span class="n">e_lengths</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">e_lstm_outputs_merged</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">e_lstm_outputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

</pre></code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Matrix multiply to get scores for each class<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Matrix multiply to get scores for each class<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Matrix multiply to get scores for each class<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">e_predictions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">(</span><span class="n">e_lstm_outputs_merged</span><span class="p">,</span>
                <span class="n">NTAGS</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Cross-entropy loss. The reduction flag is crucial (the default is to average over the sequence). The weights flag accounts for padding that makes all of the sequences the same length.<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Cross-entropy loss. The reduction flag is crucial (the default is to average over the sequence). The weights flag accounts for padding that makes all of the sequences the same length.<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Cross-entropy loss. The reduction flag is crucial (the default is to average over the sequence). The weights flag accounts for padding that makes all of the sequences the same length.<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">e_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy</span><span class="p">(</span><span class="n">e_gold_output</span><span class="p">,</span>
                <span class="n">e_predictions</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">e_mask</span><span class="p">,</span>
                <span class="n">reduction</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">Reduction</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Update computation - one step option<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Update computation - one step option<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Update computation - one step option<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">e_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">e_learning_rate</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">e_loss</span><span class="p">)</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Update computation - multi-step, so that (for example) gradient clipping can be applied<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Update computation - multi-step, so that (for example) gradient clipping can be applied<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Update computation - multi-step, so that (for example) gradient clipping can be applied<br /><br /></span><code class="tensorflow"><pre><span></span>        <span class="c1"># e_optimiser = tf.train.GradientDescentOptimizer(LEARNING_RATE)</span>
        <span class="c1"># e_gradients = e_optimiser.compute_gradients(e_loss)</span>
        <span class="c1"># e_clipped_gradients = [(tf.clip_by_value(grad, -5., 5.), var)</span>
        <span class="c1">#         for grad, var in e_gradients]</span>
        <span class="c1"># e_train = e_optimiser.apply_gradients(e_gradients)</span>

</pre></code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Get the predicted label<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Get the predicted label<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Get the predicted label<br /><br /></span><code class="tensorflow"><pre><span></span>        <span class="n">e_auto_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">e_predictions</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

</pre></code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Use computation graph<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Use computation graph<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Use computation graph<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Configure the system environment. By default tensorflow uses all available GPUs and RAM. These lines limit the number of GPUs used and the amount of RAM. To limit which GPUs are used, set the environment variable CUDA_VISIBLE_DEVICES (e.g. "export CUDA_VISIBLE_DEVICES=0,1").<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Configure the system environment. By default tensorflow uses all available GPUs and RAM. These lines limit the number of GPUs used and the amount of RAM. To limit which GPUs are used, set the environment variable CUDA_VISIBLE_DEVICES (e.g. "export CUDA_VISIBLE_DEVICES=0,1").<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Configure the system environment. By default tensorflow uses all available GPUs and RAM. These lines limit the number of GPUs used and the amount of RAM. To limit which GPUs are used, set the environment variable CUDA_VISIBLE_DEVICES (e.g. "export CUDA_VISIBLE_DEVICES=0,1").<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">config</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">(</span>
            <span class="n">device_count</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;GPU&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span>
            <span class="n">gpu_options</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">GPUOptions</span><span class="p">(</span><span class="n">per_process_gpu_memory_fraction</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Initialise all variables<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Initialise all variables<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Initialise all variables<br /><br /></span><code class="tensorflow"><pre><span></span>            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content left">Main training loop, in which we shuffle the data, set the learning rate, do one complete pass over the training data, then evaluate on the development data.
<br />
To make the code match across the three versions, we group together some framework specifc values needed when doing a pass over the data.<br /><br /></span><code class="dynet"><pre><span></span>    <span class="n">expressions</span> <span class="o">=</span> <span class="p">(</span><span class="n">pEmbedding</span><span class="p">,</span> <span class="n">pOutput</span><span class="p">,</span> <span class="n">f_lstm</span><span class="p">,</span> <span class="n">b_lstm</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
        <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>

</pre></code><span class="description shared-content centre">Main training loop, in which we shuffle the data, set the learning rate, do one complete pass over the training data, then evaluate on the development data.
<br />
To make the code match across the three versions, we group together some framework specifc values needed when doing a pass over the data.<br /><br /></span><code class="pytorch"><pre><span></span>    <span class="n">expressions</span> <span class="o">=</span> <span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
        <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>

</pre></code><span class="description shared-content right">Main training loop, in which we shuffle the data, set the learning rate, do one complete pass over the training data, then evaluate on the development data.
<br />
To make the code match across the three versions, we group together some framework specifc values needed when doing a pass over the data.<br /><br /></span><code class="tensorflow"><pre><span></span>            <span class="n">expressions</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">e_auto_output</span><span class="p">,</span> <span class="n">e_gold_output</span><span class="p">,</span> <span class="n">e_input</span><span class="p">,</span> <span class="n">e_keep_prob</span><span class="p">,</span> <span class="n">e_lengths</span><span class="p">,</span>
                <span class="n">e_loss</span><span class="p">,</span> <span class="n">e_train</span><span class="p">,</span> <span class="n">e_mask</span><span class="p">,</span> <span class="n">e_learning_rate</span><span class="p">,</span> <span class="n">sess</span>
            <span class="p">]</span>
            <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
                <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content left">Determine the current learning rate<br /><br /></span><code class="dynet"><pre><span></span>        <span class="n">trainer</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span> <span class="n">LEARNING_DECAY_RATE</span> <span class="o">*</span> <span class="n">epoch</span><span class="p">)</span>

</pre></code><span class="description shared-content centre">Determine the current learning rate<br /><br /></span><code class="pytorch"><pre><span></span>        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

</pre></code><span class="description shared-content right">Determine the current learning rate<br /><br /></span><code class="tensorflow"><pre><span></span>                <span class="n">current_lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span> <span class="n">LEARNING_DECAY_RATE</span> <span class="o">*</span> <span class="n">epoch</span><span class="p">)</span>

</pre></code></div>
<div class="outer pytorch">
<span class="description pytorch left">Set in training mode, which does things like enable dropout components, and initialise the gradient to zero.<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Set in training mode, which does things like enable dropout components, and initialise the gradient to zero.<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> 
        <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</code><span class="description pytorch right">Set in training mode, which does things like enable dropout components, and initialise the gradient to zero.<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer shared-content">
<span class="description shared-content left">Training pass<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>        <span class="n">loss</span><span class="p">,</span> <span class="n">tacc</span> <span class="o">=</span> <span class="n">do_pass</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span>
                <span class="n">current_lr</span><span class="p">)</span>
</pre></div>
</code><span class="description shared-content centre">Training pass<br /><br /></span><code class="pytorch"><pre><span></span>        <span class="n">loss</span><span class="p">,</span> <span class="n">tacc</span> <span class="o">=</span> <span class="n">do_pass</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span>
                <span class="bp">True</span><span class="p">)</span>

</pre></code><span class="description shared-content right">Training pass<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>                <span class="n">loss</span><span class="p">,</span> <span class="n">tacc</span> <span class="o">=</span> <span class="n">do_pass</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span>
                        <span class="n">expressions</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="n">current_lr</span><span class="p">)</span>
</pre></div>
</code></div>
<div class="outer pytorch">
<span class="description pytorch left">Set in evaluation mode, which does things like disable dropout components<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Set in evaluation mode, which does things like disable dropout components<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</code><span class="description pytorch right">Set in evaluation mode, which does things like disable dropout components<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer shared-content">
<span class="description shared-content left">Dev pass<br /><br /></span><code class="dynet"><pre><span></span>        <span class="n">_</span><span class="p">,</span> <span class="n">dacc</span> <span class="o">=</span> <span class="n">do_pass</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;{} loss {} t-acc {} d-acc {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">tacc</span><span class="p">,</span> <span class="n">dacc</span><span class="p">))</span>

</pre></code><span class="description shared-content centre">Dev pass<br /><br /></span><code class="pytorch"><pre><span></span>        <span class="n">_</span><span class="p">,</span> <span class="n">dacc</span> <span class="o">=</span> <span class="n">do_pass</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;{} loss {} t-acc {} d-acc {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span>
            <span class="n">tacc</span><span class="p">,</span> <span class="n">dacc</span><span class="p">))</span>

</pre></code><span class="description shared-content right">Dev pass<br /><br /></span><code class="tensorflow"><pre><span></span>                <span class="n">_</span><span class="p">,</span> <span class="n">dacc</span> <span class="o">=</span> <span class="n">do_pass</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span>
                        <span class="bp">False</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="s2">&quot;{} loss {} t-acc {} d-acc {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">tacc</span><span class="p">,</span>
                    <span class="n">dacc</span><span class="p">))</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content left">Save and load model. Both must be done after the definitions above (ie, the model should be recreated, then have its parameters set to match this saved version).<br /><br /></span><code class="dynet"><pre><span></span>    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;tagger.dy.model&quot;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">populate</span><span class="p">(</span><span class="s2">&quot;tagger.dy.model&quot;</span><span class="p">)</span>

</pre></code><span class="description shared-content centre">Save and load model. Both must be done after the definitions above (ie, the model should be recreated, then have its parameters set to match this saved version).<br /><br /></span><code class="pytorch"><pre><span></span>    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">&quot;tagger.pt.model&quot;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tagger.pt.model&#39;</span><span class="p">))</span>

</pre></code><span class="description shared-content right">Save and load model. Both must be done after the definitions above (ie, the model should be recreated, then have its parameters set to match this saved version).<br /><br /></span><code class="tensorflow"><pre><span></span>            <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./tagger.tf.model&quot;</span><span class="p">)</span>
            <span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;./tagger.tf.model&quot;</span><span class="p">)</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content left">Evaluation<br /><br /></span><code class="dynet"><pre><span></span>    <span class="n">_</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">do_pass</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Test Accuracy: {:.3f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_acc</span><span class="p">))</span>

</pre></code><span class="description shared-content centre">Evaluation<br /><br /></span><code class="pytorch"><pre><span></span>    <span class="n">_</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">do_pass</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Test Accuracy: {:.3f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_acc</span><span class="p">))</span>

</pre></code><span class="description shared-content right">Evaluation<br /><br /></span><code class="tensorflow"><pre><span></span>            <span class="n">_</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">do_pass</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span>
                    <span class="bp">False</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Test Accuracy: {:.3f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_acc</span><span class="p">))</span>

</pre></code></div>
<div class="outer pytorch">
<span class="description pytorch left">Neural network definition code. In PyTorch networks are defined using classes that extend Module.<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Neural network definition code. In PyTorch networks are defined using classes that extend Module.<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span><span class="k">class</span> <span class="nc">TaggerModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</pre></div>
</code><span class="description pytorch right">Neural network definition code. In PyTorch networks are defined using classes that extend Module.<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">In the constructor we define objects that will do each of the computations<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">In the constructor we define objects that will do each of the computations<br /><br /></span><code class="pytorch"><pre><span></span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nwords</span><span class="p">,</span> <span class="n">ntags</span><span class="p">,</span> <span class="n">pretrained_list</span><span class="p">,</span> <span class="n">id_to_token</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

</pre></code><span class="description pytorch right">In the constructor we define objects that will do each of the computations<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">Convert the word embeddings into a PyTorch tensor<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Convert the word embeddings into a PyTorch tensor<br /><br /></span><code class="pytorch"><pre><span></span>        <span class="n">pretrained_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">pretrained_list</span><span class="p">)</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">word_embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                <span class="n">pretrained_tensor</span><span class="p">,</span> <span class="n">freeze</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">KEEP_PROB</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">DIM_EMBEDDING</span><span class="p">,</span> <span class="n">LSTM_HIDDEN</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm_output_dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">KEEP_PROB</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_to_tag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">LSTM_HIDDEN</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">ntags</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">cur_batch_size</span><span class="p">):</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="n">sentences</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

</pre></code><span class="description pytorch right">Convert the word embeddings into a PyTorch tensor<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">Look up word vectors<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Look up word vectors<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">word_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_embedding</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
</pre></div>
</code><span class="description pytorch right">Look up word vectors<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">Apply dropout<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Apply dropout<br /><br /></span><code class="pytorch"><pre><span></span>        <span class="n">dropped_word_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_dropout</span><span class="p">(</span><span class="n">word_vectors</span><span class="p">)</span>

</pre></code><span class="description pytorch right">Apply dropout<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">Assuming the data is ordered longest to shortest, this provides a view of the data that fits with how cuDNN works<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Assuming the data is ordered longest to shortest, this provides a view of the data that fits with how cuDNN works<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">packed_words</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pack_padded_sequence</span><span class="p">(</span>
                <span class="n">dropped_word_vectors</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
</pre></div>
</code><span class="description pytorch right">Assuming the data is ordered longest to shortest, this provides a view of the data that fits with how cuDNN works<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">Run the LSTM over the input<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Run the LSTM over the input<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">lstm_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">packed_words</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
</pre></div>
</code><span class="description pytorch right">Run the LSTM over the input<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">Reverse the view shift made for cuDNN. Specifying total_length is not necessary in general (it can be inferred), but is necessary for parallel processing.<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Reverse the view shift made for cuDNN. Specifying total_length is not necessary in general (it can be inferred), but is necessary for parallel processing.<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">lstm_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_packed_sequence</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">,</span>
                <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">total_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span>
</pre></div>
</code><span class="description pytorch right">Reverse the view shift made for cuDNN. Specifying total_length is not necessary in general (it can be inferred), but is necessary for parallel processing.<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">Apply dropout to the output<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Apply dropout to the output<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">lstm_out_dropped</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm_output_dropout</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">)</span>
</pre></div>
</code><span class="description pytorch right">Apply dropout to the output<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">Matrix multiply to get distribution over tags<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Matrix multiply to get distribution over tags<br /><br /></span><code class="pytorch"><pre><span></span>        <span class="n">output_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_to_tag</span><span class="p">(</span><span class="n">lstm_out_dropped</span><span class="p">)</span>

</pre></code><span class="description pytorch right">Matrix multiply to get distribution over tags<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">Reshape to [batch size * sequence length, ntags] for more efficient processing<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Reshape to [batch size * sequence length, ntags] for more efficient processing<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">output_scores</span> <span class="o">=</span> <span class="n">output_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">cur_batch_size</span> <span class="o">*</span> <span class="n">max_length</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">flat_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">cur_batch_size</span> <span class="o">*</span> <span class="n">max_length</span><span class="p">)</span>
</pre></div>
</code><span class="description pytorch right">Reshape to [batch size * sequence length, ntags] for more efficient processing<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">Calculate the cross entropy loss, ignoring padding, and summing losses across the batch<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Calculate the cross entropy loss, ignoring padding, and summing losses across the batch<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">loss_function</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">output_scores</span><span class="p">,</span> <span class="n">flat_labels</span><span class="p">)</span>
</pre></div>
</code><span class="description pytorch right">Calculate the cross entropy loss, ignoring padding, and summing losses across the batch<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">Identify the highest scoring tag in each case and reshape to be [batch, sequence]<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Identify the highest scoring tag in each case and reshape to be [batch, sequence]<br /><br /></span><code class="pytorch"><pre><span></span>        <span class="n">_</span><span class="p">,</span> <span class="n">predicted_tags</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output_scores</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">predicted_tags</span> <span class="o">=</span> <span class="n">predicted_tags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">cur_batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">predicted_tags</span>

</pre></code><span class="description pytorch right">Identify the highest scoring tag in each case and reshape to be [batch, sequence]<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer shared-content">
<span class="description shared-content left">Inference (the same function for train and test)<br /><br /></span><code class="dynet"><pre><span></span><span class="k">def</span> <span class="nf">do_pass</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span> <span class="n">train</span><span class="p">):</span>
    <span class="n">pEmbedding</span><span class="p">,</span> <span class="n">pOutput</span><span class="p">,</span> <span class="n">f_lstm</span><span class="p">,</span> <span class="n">b_lstm</span><span class="p">,</span> <span class="n">trainer</span> <span class="o">=</span> <span class="n">expressions</span>

</pre></code><span class="description shared-content centre">Inference (the same function for train and test)<br /><br /></span><code class="pytorch"><pre><span></span><span class="k">def</span> <span class="nf">do_pass</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span> <span class="n">train</span><span class="p">):</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">expressions</span>

</pre></code><span class="description shared-content right">Inference (the same function for train and test)<br /><br /></span><code class="tensorflow"><pre><span></span><span class="k">def</span> <span class="nf">do_pass</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">token_to_id</span><span class="p">,</span> <span class="n">tag_to_id</span><span class="p">,</span> <span class="n">expressions</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
    <span class="n">e_auto_output</span><span class="p">,</span> <span class="n">e_gold_output</span><span class="p">,</span> <span class="n">e_input</span><span class="p">,</span> <span class="n">e_keep_prob</span><span class="p">,</span> <span class="n">e_lengths</span><span class="p">,</span> <span class="n">e_loss</span><span class="p">,</span> \
            <span class="n">e_train</span><span class="p">,</span> <span class="n">e_mask</span><span class="p">,</span> <span class="n">e_learning_rate</span><span class="p">,</span> <span class="n">session</span> <span class="o">=</span> <span class="n">expressions</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content left">Loop over batches, tracking the start of the batch in the data<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">match</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
</pre></div>
</code><span class="description shared-content centre">Loop over batches, tracking the start of the batch in the data<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">match</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
</pre></div>
</code><span class="description shared-content right">Loop over batches, tracking the start of the batch in the data<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">match</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content left">Form the batch and order it based on length (not necessary for DyNet, but important for efficient processing in PyTorch).<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>        <span class="n">batch</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">start</span> <span class="p">:</span> <span class="n">start</span> <span class="o">+</span> <span class="n">BATCH_SIZE</span><span class="p">]</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">start</span> <span class="o">+=</span> <span class="n">BATCH_SIZE</span>
</pre></div>
</code><span class="description shared-content centre">Form the batch and order it based on length (not necessary for DyNet, but important for efficient processing in PyTorch).<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">batch</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">start</span> <span class="p">:</span> <span class="n">start</span> <span class="o">+</span> <span class="n">BATCH_SIZE</span><span class="p">]</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">start</span> <span class="o">+=</span> <span class="n">BATCH_SIZE</span>
</pre></div>
</code><span class="description shared-content right">Form the batch and order it based on length (not necessary for DyNet, but important for efficient processing in PyTorch).<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">batch</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">start</span> <span class="p">:</span> <span class="n">start</span> <span class="o">+</span> <span class="n">BATCH_SIZE</span><span class="p">]</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">start</span> <span class="o">+=</span> <span class="n">BATCH_SIZE</span>
</pre></div>
</code></div>
<div class="outer shared-content">
<span class="description shared-content left">Log partial results so we can conveniently check progress<br /><br /></span><code class="dynet"><pre><span></span>        <span class="k">if</span> <span class="n">start</span> <span class="o">%</span> <span class="mi">4000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">match</span> <span class="o">/</span> <span class="n">total</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>

</pre></code><span class="description shared-content centre">Log partial results so we can conveniently check progress<br /><br /></span><code class="pytorch"><pre><span></span>        <span class="k">if</span> <span class="n">start</span> <span class="o">%</span> <span class="mi">4000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">match</span> <span class="o">/</span> <span class="n">total</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>

</pre></code><span class="description shared-content right">Log partial results so we can conveniently check progress<br /><br /></span><code class="tensorflow"><pre><span></span>        <span class="k">if</span> <span class="n">start</span> <span class="o">%</span> <span class="mi">4000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">match</span> <span class="o">/</span> <span class="n">total</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>

</pre></code></div>
<div class="outer dynet">
<span class="description dynet left">Start a new computation graph for this batch<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>        <span class="n">dy</span><span class="o">.</span><span class="n">renew_cg</span><span class="p">()</span>
</pre></div>
</code><span class="description dynet centre">Start a new computation graph for this batch<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">Start a new computation graph for this batch<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet left">For each example, we will construct an expression that gives the loss.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>        <span class="n">loss_expressions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">predicted</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">tags</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
</pre></div>
</code><span class="description dynet centre">For each example, we will construct an expression that gives the loss.<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">For each example, we will construct an expression that gives the loss.<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet left">Convert tokens and tags from strings to numbers using the indices<br /><br /></span><code class="dynet"><pre><span></span>            <span class="n">token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">token_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">simplify_token</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
            <span class="n">tag_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tag_to_id</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tags</span><span class="p">]</span>

</pre></code><span class="description dynet centre">Convert tokens and tags from strings to numbers using the indices<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">Convert tokens and tags from strings to numbers using the indices<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet left">Look up word embeddings<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>            <span class="n">wembs</span> <span class="o">=</span> <span class="p">[</span><span class="n">dy</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">pEmbedding</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">token_ids</span><span class="p">]</span>
</pre></div>
</code><span class="description dynet centre">Look up word embeddings<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">Look up word embeddings<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet left">During training, apply dropout to the inputs<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>            <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
                <span class="n">wembs</span> <span class="o">=</span> <span class="p">[</span><span class="n">dy</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">KEEP_PROB</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">wembs</span><span class="p">]</span>
</pre></div>
</code><span class="description dynet centre">During training, apply dropout to the inputs<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">During training, apply dropout to the inputs<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet left">Create an expression for two LSTMs and feed in the embeddings (reversed in one case).
<br />
We pull out the output vector from the cell state at each step.<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>            <span class="n">f_init</span> <span class="o">=</span> <span class="n">f_lstm</span><span class="o">.</span><span class="n">initial_state</span><span class="p">()</span>
            <span class="n">f_lstm_output</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">output</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">f_init</span><span class="o">.</span><span class="n">add_inputs</span><span class="p">(</span><span class="n">wembs</span><span class="p">)]</span>
            <span class="n">rev_embs</span> <span class="o">=</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">wembs</span><span class="p">)</span>
            <span class="n">b_init</span> <span class="o">=</span> <span class="n">b_lstm</span><span class="o">.</span><span class="n">initial_state</span><span class="p">()</span>
            <span class="n">b_lstm_output</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">output</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">b_init</span><span class="o">.</span><span class="n">add_inputs</span><span class="p">(</span><span class="n">rev_embs</span><span class="p">)]</span>

            <span class="n">pred_tags</span> <span class="o">=</span> <span class="p">[]</span>
</pre></div>
</code><span class="description dynet centre">Create an expression for two LSTMs and feed in the embeddings (reversed in one case).
<br />
We pull out the output vector from the cell state at each step.<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">Create an expression for two LSTMs and feed in the embeddings (reversed in one case).
<br />
We pull out the output vector from the cell state at each step.<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet left">Combine the outputs<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>            <span class="k">for</span> <span class="n">f</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">f_lstm_output</span><span class="p">,</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">b_lstm_output</span><span class="p">),</span> <span class="n">tag_ids</span><span class="p">):</span>
                <span class="n">combined</span> <span class="o">=</span> <span class="n">dy</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">f</span><span class="p">,</span><span class="n">b</span><span class="p">])</span>
</pre></div>
</code><span class="description dynet centre">Combine the outputs<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">Combine the outputs<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet left">Apply dropout to the output<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>                <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
                    <span class="n">combined</span> <span class="o">=</span> <span class="n">dy</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">combined</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">KEEP_PROB</span><span class="p">)</span>
</pre></div>
</code><span class="description dynet centre">Apply dropout to the output<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">Apply dropout to the output<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet left">Multiple by a matrix to get scores for each tag<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>                <span class="n">r_t</span> <span class="o">=</span> <span class="n">pOutput</span> <span class="o">*</span> <span class="n">combined</span>
                <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
</pre></div>
</code><span class="description dynet centre">Multiple by a matrix to get scores for each tag<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">Multiple by a matrix to get scores for each tag<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet left">When training, get an expression for the cross-entropy loss<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>                    <span class="n">err</span> <span class="o">=</span> <span class="n">dy</span><span class="o">.</span><span class="n">pickneglogsoftmax</span><span class="p">(</span><span class="n">r_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
                    <span class="n">loss_expressions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
</pre></div>
</code><span class="description dynet centre">When training, get an expression for the cross-entropy loss<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">When training, get an expression for the cross-entropy loss<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet left">Calculate the highest scoring tag (which will lead to evaluation of the graph)<br /><br /></span><code class="dynet"><pre><span></span>                <span class="n">chosen</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">r_t</span><span class="o">.</span><span class="n">npvalue</span><span class="p">())</span>
                <span class="n">pred_tags</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">chosen</span><span class="p">)</span>
            <span class="n">predicted</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_tags</span><span class="p">)</span>

</pre></code><span class="description dynet centre">Calculate the highest scoring tag (which will lead to evaluation of the graph)<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">Calculate the highest scoring tag (which will lead to evaluation of the graph)<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer dynet">
<span class="description dynet left">During training, combine the losses for the batch, do an update, and record the loss<br /><br /></span><code class="dynet"><pre><span></span>        <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
            <span class="n">loss_for_batch</span> <span class="o">=</span> <span class="n">dy</span><span class="o">.</span><span class="n">esum</span><span class="p">(</span><span class="n">loss_expressions</span><span class="p">)</span>
            <span class="n">loss_for_batch</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">loss_for_batch</span><span class="o">.</span><span class="n">scalar_value</span><span class="p">()</span>

</pre></code><span class="description dynet centre">During training, combine the losses for the batch, do an update, and record the loss<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description dynet right">During training, combine the losses for the batch, do an update, and record the loss<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">Prepare input arrays, using .long() to cast the type from Tensor to LongTensor.<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Prepare input arrays, using .long() to cast the type from Tensor to LongTensor.<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>        <span class="n">cur_batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
        <span class="n">input_array</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">cur_batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">))</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
        <span class="n">output_array</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">cur_batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">))</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">tags</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
</pre></div>
</code><span class="description pytorch right">Prepare input arrays, using .long() to cast the type from Tensor to LongTensor.<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">Using the indices we map our srings to numbers.<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Using the indices we map our srings to numbers.<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>            <span class="n">token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">token_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">simplify_token</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
            <span class="n">tag_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tag_to_id</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tags</span><span class="p">]</span>
</pre></div>
</code><span class="description pytorch right">Using the indices we map our srings to numbers.<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">Fill the arrays, leaving the remaining values as zero (our padding value).<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Fill the arrays, leaving the remaining values as zero (our padding value).<br /><br /></span><code class="pytorch"><pre><span></span>            <span class="n">input_array</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
            <span class="n">output_array</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">tags</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">tag_ids</span><span class="p">)</span>

</pre></code><span class="description pytorch right">Fill the arrays, leaving the remaining values as zero (our padding value).<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">Calling the model as a function will run its forward() function, which constructs the network.<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Calling the model as a function will run its forward() function, which constructs the network.<br /><br /></span><code class="pytorch"><pre><span></span>        <span class="n">batch_loss</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_array</span><span class="p">,</span> <span class="n">output_array</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span>
                <span class="n">cur_batch_size</span><span class="p">)</span>

</pre></code><span class="description pytorch right">Calling the model as a function will run its forward() function, which constructs the network.<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">In training we do the backwards pass, apply the update, and reset the gradient.<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">In training we do the backwards pass, apply the update, and reset the gradient.<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>        <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
            <span class="n">batch_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</code><span class="description pytorch right">In training we do the backwards pass, apply the update, and reset the gradient.<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">To get the loss value we use .item().<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">To get the loss value we use .item().<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>            <span class="n">loss</span> <span class="o">+=</span> <span class="n">batch_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</code><span class="description pytorch right">To get the loss value we use .item().<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer pytorch">
<span class="description pytorch left">Our output is an array (rather than a single value), so we use a different approach to get it into a usable form.<br /><br /></span><code class="dynet">&nbsp;</code><span class="description pytorch centre">Our output is an array (rather than a single value), so we use a different approach to get it into a usable form.<br /><br /></span><code class="pytorch"><pre><span></span>        <span class="n">predicted</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

</pre></code><span class="description pytorch right">Our output is an array (rather than a single value), so we use a different approach to get it into a usable form.<br /><br /></span><code class="tensorflow">&nbsp;</code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Add empty sentences to fill in batch<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Add empty sentences to fill in batch<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Add empty sentences to fill in batch<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">batch</span> <span class="o">+=</span> <span class="p">[([],</span> <span class="p">[])</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">BATCH_SIZE</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">))]</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Prepare input. We do this here for convenience and to have greater alignment between code above, but in practise it would be best to do this once in pre-processing.<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Prepare input. We do this here for convenience and to have greater alignment between code above, but in practise it would be best to do this once in pre-processing.<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Prepare input. We do this here for convenience and to have greater alignment between code above, but in practise it would be best to do this once in pre-processing.<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">max_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">input_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="n">max_length</span><span class="p">])</span>
        <span class="n">output_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="n">max_length</span><span class="p">])</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">])</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="n">max_length</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">tags</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Using the indices we map our srings to numbers<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Using the indices we map our srings to numbers<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Using the indices we map our srings to numbers<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>            <span class="n">token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">token_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">simplify_token</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
            <span class="n">tag_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tag_to_id</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tags</span><span class="p">]</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Fill the arrays, leaving the remaining values as zero (our padding value).<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Fill the arrays, leaving the remaining values as zero (our padding value).<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Fill the arrays, leaving the remaining values as zero (our padding value).<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>            <span class="n">input_array</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)]</span> <span class="o">=</span> <span class="n">token_ids</span>
            <span class="n">output_array</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">tags</span><span class="p">)]</span> <span class="o">=</span> <span class="n">tag_ids</span>
            <span class="n">mask</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)])</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">We can't change the computation graph to disable dropout when not training, so we just change the keep probability.<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">We can't change the computation graph to disable dropout when not training, so we just change the keep probability.<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">We can't change the computation graph to disable dropout when not training, so we just change the keep probability.<br /><br /></span><code class="tensorflow"><pre><span></span>        <span class="n">cur_keep_prob</span> <span class="o">=</span> <span class="n">KEEP_PROB</span> <span class="k">if</span> <span class="n">train</span> <span class="k">else</span> <span class="mf">1.0</span>

</pre></code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">This dictionary contains values for all of the placeholders we defined.<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">This dictionary contains values for all of the placeholders we defined.<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">This dictionary contains values for all of the placeholders we defined.<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">feed</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">e_input</span><span class="p">:</span> <span class="n">input_array</span><span class="p">,</span>
                <span class="n">e_gold_output</span><span class="p">:</span> <span class="n">output_array</span><span class="p">,</span>
                <span class="n">e_mask</span><span class="p">:</span> <span class="n">mask</span><span class="p">,</span>
                <span class="n">e_keep_prob</span><span class="p">:</span> <span class="n">cur_keep_prob</span><span class="p">,</span>
                <span class="n">e_lengths</span><span class="p">:</span> <span class="n">lengths</span><span class="p">,</span>
                <span class="n">e_learning_rate</span><span class="p">:</span> <span class="n">lr</span>
        <span class="p">}</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Define the computations we want tensorflow to complete. If we are not training we do not need to compute a loss and we do not want to do the update.<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Define the computations we want tensorflow to complete. If we are not training we do not need to compute a loss and we do not want to do the update.<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Define the computations we want tensorflow to complete. If we are not training we do not need to compute a loss and we do not want to do the update.<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">todo</span> <span class="o">=</span> <span class="p">[</span><span class="n">e_auto_output</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
            <span class="n">todo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e_loss</span><span class="p">)</span>
            <span class="n">todo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e_train</span><span class="p">)</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Running the network<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Running the network<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Running the network<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>        <span class="n">outcomes</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">todo</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed</span><span class="p">)</span>
</pre></div>
</code></div>
<div class="outer tensorflow">
<span class="description tensorflow left">Getting our values out. Note, we do not request the e_train value because its work is done - it performed the update during its computation.<br /><br /></span><code class="dynet">&nbsp;</code><span class="description tensorflow centre">Getting our values out. Note, we do not request the e_train value because its work is done - it performed the update during its computation.<br /><br /></span><code class="pytorch">&nbsp;</code><span class="description tensorflow right">Getting our values out. Note, we do not request the e_train value because its work is done - it performed the update during its computation.<br /><br /></span><code class="tensorflow"><pre><span></span>        <span class="n">predicted</span> <span class="o">=</span> <span class="n">outcomes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">outcomes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

</pre></code></div>
<div class="outer shared-content">
<span class="description shared-content left">Update the number of correct tags and total tags<br /><br /></span><code class="dynet"><div class="source"><pre><span></span>        <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">g</span><span class="p">),</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">predicted</span><span class="p">):</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">gt</span><span class="p">,</span> <span class="n">at</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
                <span class="n">gt</span> <span class="o">=</span> <span class="n">tag_to_id</span><span class="p">[</span><span class="n">gt</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">gt</span> <span class="o">==</span> <span class="n">at</span><span class="p">:</span>
                    <span class="n">match</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">match</span> <span class="o">/</span> <span class="n">total</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</code><span class="description shared-content centre">Update the number of correct tags and total tags<br /><br /></span><code class="pytorch"><div class="source"><pre><span></span>        <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">g</span><span class="p">),</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">predicted</span><span class="p">):</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">gt</span><span class="p">,</span> <span class="n">at</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
                <span class="n">gt</span> <span class="o">=</span> <span class="n">tag_to_id</span><span class="p">[</span><span class="n">gt</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">gt</span> <span class="o">==</span> <span class="n">at</span><span class="p">:</span>
                    <span class="n">match</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">match</span> <span class="o">/</span> <span class="n">total</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</code><span class="description shared-content right">Update the number of correct tags and total tags<br /><br /></span><code class="tensorflow"><div class="source"><pre><span></span>        <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">g</span><span class="p">),</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">predicted</span><span class="p">):</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">gt</span><span class="p">,</span> <span class="n">at</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
                <span class="n">gt</span> <span class="o">=</span> <span class="n">tag_to_id</span><span class="p">[</span><span class="n">gt</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">gt</span> <span class="o">==</span> <span class="n">at</span><span class="p">:</span>
                    <span class="n">match</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">match</span> <span class="o">/</span> <span class="n">total</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</code></div>
<br /></div>


<script>
var dyShowing = false;
var tfShowing = false;
var ptShowing = false;
function whichShowing() {
    if (dyShowing && tfShowing && ptShowing) return "all";
    else if (dyShowing && ptShowing) return "-t";
    else if (dyShowing && tfShowing) return "-p";
    else if (ptShowing && tfShowing) return "-d";
    else if (dyShowing) return "d";
    else if (ptShowing) return "p";
    else if (tfShowing) return "t";
    else return "-";
}
function toggleItem(toEdit, showing) {
    if (toEdit.classList.contains('outer')) {
        if (
            (showing === "d" && toEdit.classList.contains("dynet")) ||
            (showing === "p" && toEdit.classList.contains("pytorch")) ||
            (showing === "t" && toEdit.classList.contains("tensorflow")) ||
            (showing === "-d" && toEdit.classList.contains("pytorch")) ||
            (showing === "-p" && toEdit.classList.contains("dynet")) ||
            (showing === "-t" && toEdit.classList.contains("dynet")) ||
            (showing === "-d" && toEdit.classList.contains("tensorflow")) ||
            (showing === "-p" && toEdit.classList.contains("tensorflow")) ||
            (showing === "-t" && toEdit.classList.contains("pytorch")) ||
            (showing != "-" && toEdit.classList.contains("shared-content")) ||
            showing === "all"
           ) {
            toEdit.style.display = "block";
        } else {
            toEdit.style.display = "none";
        }
    } else if (toEdit.classList.contains("left")) {
        if ((showing === "d" && toEdit.classList.contains("dynet")) ||
            (showing === "p" && toEdit.classList.contains("pytorch")) ||
            (showing === "t" && toEdit.classList.contains("tensorflow")) ||
            ((showing === "p" || showing === "t" || showing === "d") && toEdit.classList.contains("shared-content"))) {
            toEdit.style.display = "inline-block";
        } else {
            toEdit.style.display = "none";
        }
    } else if (toEdit.classList.contains("centre")) {
        if (
            (showing === "all" && (!toEdit.classList.contains("tensorflow"))) ||
            (showing === "-p" && (!toEdit.classList.contains("pytorch"))) || 
            (showing === "-t" && (!toEdit.classList.contains("tensorflow")))) {
            toEdit.style.display = "inline-block";
        } else {
            toEdit.style.display = "none";
        }
    } else if (toEdit.classList.contains("right")) {
        if (
            (showing === "-d" && (!toEdit.classList.contains("dynet"))) ||
            (showing === "all" && toEdit.classList.contains("tensorflow"))
        ) {
            toEdit.style.display = "inline-block";
        } else {
            toEdit.style.display = "none";
        }
    } else {
        if ((showing === "d" && toEdit.classList.contains("dynet")) ||
            (showing === "p" && toEdit.classList.contains("pytorch")) ||
            (showing === "t" && toEdit.classList.contains("tensorflow")) ||
            (showing === "-d" && toEdit.classList.contains("pytorch")) ||
            (showing === "-p" && toEdit.classList.contains("dynet")) ||
            (showing === "-t" && toEdit.classList.contains("dynet")) ||
            (showing === "-d" && toEdit.classList.contains("tensorflow")) ||
            (showing === "-p" && toEdit.classList.contains("tensorflow")) ||
            (showing === "-t" && toEdit.classList.contains("pytorch")) ||
            (showing != "-" && toEdit.classList.contains("shared-content")) ||
            (showing === "all")) {
            toEdit.style.display = "inline-block";
        } else {
            toEdit.style.display = "none";
        }
    }
}
function toggleDyNet() {
    dyShowing = ! dyShowing;

    var dybutton = document.getElementById("dybutton");
    if (dyShowing) dybutton.style.backgroundColor = "#f1f5a4";
    else dybutton.style.backgroundColor = "#f8fad2";

    toggleAll();
}
function togglePyTorch() {
    ptShowing = ! ptShowing;

    var ptbutton = document.getElementById("ptbutton");
    if (! ptShowing) ptbutton.style.backgroundColor = "#fbe1d3";
    else ptbutton.style.backgroundColor = "#f7c1a4";

    toggleAll();
}
function toggleTensorflow() {
    tfShowing = ! tfShowing;

    var tfbutton = document.getElementById("tfbutton");
    if (! tfShowing) tfbutton.style.backgroundColor = "#d0eaf0";
    else tfbutton.style.backgroundColor = "#a9d9e4";

    toggleAll();
}
function toggleAll() {
    showing = whichShowing();
    var dyitems = document.getElementsByClassName("dynet");
    for (var i = dyitems.length - 1; i >= 0; i--) {
        toggleItem(dyitems[i], showing);
    }
    var tfitems = document.getElementsByClassName("tensorflow");
    for (var i = tfitems.length - 1; i >= 0; i--) {
        toggleItem(tfitems[i], showing);
    }
    var ptitems = document.getElementsByClassName("pytorch");
    for (var i = ptitems.length - 1; i >= 0; i--) {
        toggleItem(ptitems[i], showing);
    }
    var allitems = document.getElementsByClassName("shared-content");
    for (var i = allitems.length - 1; i >= 0; i--) {
        toggleItem(allitems[i], showing);
    }
}
</script>

<div class="header-outer">
<div class="header">
<p>
A few miscellaneous notes:
<ul>
    <li>PyTorch 0.4 does not support recurrent dropout directly. For an example of how to achieve it, see the LSTM and QRNN Language Model Toolkit's <a href="https://github.com/salesforce/awd-lstm-lm/blob/28683b20154fce8e5812aeb6403e35010348c3ea/weight_drop.py">WeightDrop class</a> and <a href="https://github.com/salesforce/awd-lstm-lm/blob/457a422eb46e970a6aad659ca815a04b3d074d6c/model.py#L22">how it is used</a>.</li>
    <li>Tensorflow 1.9 does not support weight decay directly, but <a href="https://github.com/tensorflow/tensorflow/pull/17438">this pull request</a> appears to add support and will be part of 1.10.</li>
</ul>
</p>
<p>
I developed this code with help from many people and resources. In particular:
<ul>
    <li> <a href="https://github.com/jiesutd/NCRFpp">NCRFpp</a>, the code associated with <a href="https://arxiv.org/abs/1806.04470">Yang, Liang, and Zhang (CoLing 2018)</a>, which was my starting point for PyTorch and my reference point when trying to check performance for the others.</li>
    <li> Members of the <a href="http://web.eecs.umich.edu/~wlasecki/croma.html">CROMA Lab</a> who gave feedback during development.</li>
    <li> Guillaume Genthial's blog post about <a href="https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html">Sequence Tagging with Tensorflow</a>. </li>
    <li> The DyNet <a href="https://github.com/clab/dynet/blob/master/examples/tagger/bilstmtagger.py">example tagger</a>. </li>
</ul>
</p>
</div>
</div>

<div class="disqus">
<div id="disqus_thread"></div>
</div>
<script>
var disqus_config = function () {
    this.page.url = 'http://jkk.name/neural-tagger-tutorial/';
    this.page.identifier = '/neural-tagger-tutorial/';
    this.page.title = 'Neural Tagger Example';
};
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://www-jkk-name.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</body>
</html>
